{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shuon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\shuon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:294: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\shuon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:294: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\shuon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:294: UserWarning: \"b'.........'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\shuon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:294: UserWarning: \"b'C:'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\shuon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:294: UserWarning: \"b'c:'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\shuon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:294: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['if', 'youtube', 'doesn', 'allow', 'for', 'donations', 'in', 'your', 'area', 'just', 'go', 'to', 'teamtrees', 'org', 'to', 'donate', 'love', 'you', 'guys'], ['we', 'hit', 'it'], ['do', 'in', 'poland', 'that', 'in', 'city', 'named', 'bydgoszcz'], ['dear', 'mrbeast', 'can', 'you', 'please', 'help', 'the', 'bushfires', 'in', 'hell', 'mean', 'australia'], ['can', 'just', 'see', 'years', 'later', 'the', 'news', 'saying', 'years', 'ago', 'youtuber', 'named', 'mr', 'beast', 'planted', 'mil', 'trees', 'is', 'less', 'than', 'months', 'it', 'was', 'and', 'still', 'is', 'the', 'biggest', 'fundraiser', 'in', 'the', 'world'], ['ok', 'we', 'donated', 'million', 'dollars', 'but', 'who', 'gonna', 'plant', 'them', 'all', 'don', 'think', 'that', 'possible'], ['faith', 'in', 'humanity', 'went'], ['ecosia', 'is', 'the', 'answer', 'also', 'you', 'could', 'get', 'lots', 'of', 'seeds', 'from', 'lemons', 'pomegranates', 'apples', 'the', 'list', 'goes', 'on', 'on', 'there', 'are', 'currently', 'trillion', 'trees', 'want', 'there', 'to', 'be', 'trillion', 'trees', 'in', 'the', 'world'], ['we', 'need', 'mr', 'beasts', 'help', 'in', 'australia'], ['mrbeast', 'plants', 'million', 'treesww', 'gonna', 'distroy', 'this', 'man', 'whole', 'career']]\n",
      "lemmatized\n",
      "[[('allow', 1), ('area', 1), ('donation', 1), ('go', 1), ('guy', 1), ('love', 1)], [], [('name', 1)], [('bushfire', 1), ('dear', 1), ('help', 1), ('mean', 1), ('mrbeast', 1)], [('name', 1), ('big', 1), ('fundrais', 1), ('later', 1), ('less', 1), ('mil', 1), ('month', 1), ('news', 1), ('plant', 1), ('saying', 1), ('see', 1), ('still', 1), ('tree', 1), ('world', 1), ('year', 1), ('youtuber', 1)], [('go', 1), ('plant', 1), ('dollar', 1), ('donate', 1), ('possible', 1), ('think', 1)], [('go', 1), ('faith', 1), ('humanity', 1)], [('go', 1), ('tree', 2), ('world', 1), ('also', 1), ('apple', 1), ('could', 1), ('currently', 1), ('lemon', 1), ('list', 1), ('lot', 1), ('pomegranate', 1), ('seed', 1), ('want', 1)], [('help', 1), ('need', 1)], [('go', 1), ('plant', 1), ('distroy', 1), ('man', 1), ('treesww', 1)]]\n",
      "[(0,\n",
      "  '0.101*\"tree\" + 0.038*\"plant\" + 0.037*\"go\" + 0.033*\"make\" + 0.024*\"would\" + '\n",
      "  '0.022*\"think\" + 0.021*\"say\" + 0.019*\"people\" + 0.014*\"want\" + 0.013*\"team\"'),\n",
      " (1,\n",
      "  '0.046*\"video\" + 0.022*\"get\" + 0.014*\"thing\" + 0.013*\"time\" + 0.012*\"look\" + '\n",
      "  '0.012*\"even\" + 0.011*\"watch\" + 0.010*\"nice\" + 0.010*\"need\" + 0.009*\"first\"'),\n",
      " (2,\n",
      "  '0.028*\"love\" + 0.025*\"teamtree\" + 0.023*\"good\" + 0.020*\"see\" + '\n",
      "  '0.017*\"great\" + 0.016*\"know\" + 0.016*\"thank\" + 0.015*\"much\" + 0.015*\"gun\" + '\n",
      "  '0.013*\"help\"')]\n",
      "   Topic_Num  Topic_Perc_Contrib  \\\n",
      "0        0.0              0.9910   \n",
      "1        1.0              0.9002   \n",
      "2        2.0              0.9707   \n",
      "\n",
      "                                            Keywords  \\\n",
      "0  tree, plant, go, make, would, think, say, peop...   \n",
      "1  video, get, thing, time, look, even, watch, ni...   \n",
      "2  love, teamtree, good, see, great, know, thank,...   \n",
      "\n",
      "                                                Text  \n",
      "0  [tree, tree, tree, tree, tree, tree, tree, tre...  \n",
      "1  [awesome, awesome, awesome, awesome, awesome, ...  \n",
      "2  [teamtree, teamtree, teamtree, teamtree, teamt...  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>tree, plant, go, make, would, think, say, peop...</td>\n",
       "      <td>[tree, tree, tree, tree, tree, tree, tree, tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9002</td>\n",
       "      <td>video, get, thing, time, look, even, watch, ni...</td>\n",
       "      <td>[awesome, awesome, awesome, awesome, awesome, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9707</td>\n",
       "      <td>love, teamtree, good, see, great, know, thank,...</td>\n",
       "      <td>[teamtree, teamtree, teamtree, teamtree, teamt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0        0.0              0.9910   \n",
       "1        1.0              0.9002   \n",
       "2        2.0              0.9707   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  tree, plant, go, make, would, think, say, peop...   \n",
       "1  video, get, thing, time, look, even, watch, ni...   \n",
       "2  love, teamtree, good, see, great, know, thank,...   \n",
       "\n",
       "                                                Text  \n",
       "0  [tree, tree, tree, tree, tree, tree, tree, tre...  \n",
       "1  [awesome, awesome, awesome, awesome, awesome, ...  \n",
       "2  [teamtree, teamtree, teamtree, teamtree, teamt...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "        \n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row=row[0]\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "youtuber_list_path=\"subscriber_network.json\"\n",
    "\n",
    "comments=[]\n",
    "with open(youtuber_list_path,'r') as fp:\n",
    "    youtubers=json.loads(fp.read())[\"nodes\"]\n",
    "    for you in youtubers:\n",
    "        for vi in you[\"video_list\"]:\n",
    "            for comment in vi[\"comment_list\"]:\n",
    "                cleantext = BeautifulSoup(comment[\"text\"],\"html.parser\").text\n",
    "                comments.append(cleantext)\n",
    "comment_words = list(sent_to_words(comments))\n",
    "data_words = list(sent_to_words(comment_words))\n",
    "print(data_words[0:10])\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100) \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(\"lemmatized\")\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:10]])\n",
    "\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.show_topics(formatted=True))\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_lemmatized)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# # Show\n",
    "# df_dominant_topic.head(500)\n",
    "\n",
    "# youtuber_save_path=\"comment_data.csv\"\n",
    "# with open(youtuber_save_path, mode='w+', encoding=\"utf-8\", newline='') as fp:\n",
    "#     csv_writer = csv.writer(fp, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#     first = True\n",
    "#     for v in youtubers:\n",
    "#         if first:\n",
    "#             csv_writer.writerow([key for key, value in df_dominant_topic.])\n",
    "#             first = False\n",
    "#         csv_writer.writerow([value for key, value in v.items()])\n",
    "\n",
    "# # Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "print(sent_topics_sorteddf_mallet.head())\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()\n",
    "\n",
    "\n",
    "# # Number of Documents for Each Topic\n",
    "# topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# print(\"topic_counts\")\n",
    "# print(topic_counts)\n",
    "\n",
    "# # Percentage of Documents for Each Topic\n",
    "# topic_contribution = round(topic_counts/topic_counts.sum(), 5)\n",
    "\n",
    "# print(\"topic_contribution\")\n",
    "# print(topic_contribution)\n",
    "\n",
    "# # Topic Number and Keywords\n",
    "# topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# # Concatenate Column wise\n",
    "# df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# # Change Column names\n",
    "# df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# # # Show\n",
    "# # df_dominant_topics\n",
    "# # # Visualize the topics\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "# vis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
