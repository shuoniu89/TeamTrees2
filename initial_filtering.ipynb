{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file reads the raw file in and filter all non-english videos and videos published in countries other than \"US\", \"CA\", \"GB\", \"AU\", \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Cat\n",
      "[nltk_data]     Mai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from pprint import pprint\n",
    "import math\n",
    "import os \n",
    "from scipy.stats import sem\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models import Phrases\n",
    "from gensim import corpora, models\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:26: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:26: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:26: DeprecationWarning: invalid escape sequence \\.\n",
      "<ipython-input-13-495bccfd7225>:26: DeprecationWarning: invalid escape sequence \\.\n",
      "  result=re.sub('[,\\.!?]', '', str(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1445\n",
      "937\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:-5VBe9i4HDg</td>\n",
       "      <td>live tree planting counter - teamtrees [20 mil...</td>\n",
       "      <td>[live, tree, plant, counter, teamtrees, millio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:-99jZt79aWk</td>\n",
       "      <td>why are trees so awesome #teamtrees nanodots m...</td>\n",
       "      <td>[tree, awesome, teamtrees, nanodots, magnets, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:-BqH2PKqFpM</td>\n",
       "      <td>the tree face #teamtrees robisplays teamtrees ...</td>\n",
       "      <td>[tree, face, teamtrees, robisplays, teamtrees,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>:-BRWSaYQhM0</td>\n",
       "      <td>donate to teamtrees #teamtrees</td>\n",
       "      <td>[donate, teamtrees, teamtrees]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>:-cPdImejxEQ</td>\n",
       "      <td>planting 20000000 trees will actually have thi...</td>\n",
       "      <td>[plant, 20000000, tree, actually, impact, mrbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>:_rQrNfRww60</td>\n",
       "      <td>hallowtree | donate to team trees | Animation ...</td>\n",
       "      <td>[hallowtree, donate, team, tree, animation, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>:_vQ7OeVWBrU</td>\n",
       "      <td>let's stream for teamtrees gaming pixelfire ai...</td>\n",
       "      <td>[let, stream, teamtrees, game, pixelfire, code...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>:_yKFJm75qO0</td>\n",
       "      <td>planting 20 million trees in prison architect ...</td>\n",
       "      <td>[plant, million, tree, prison, architect, simu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>:_zT8aOvNLaY</td>\n",
       "      <td>asmr | bob ross inspired painting asmr for #te...</td>\n",
       "      <td>[asmr, bob, ross, inspire, paint, asmr, teamtr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>:__8RrX7Z8iY</td>\n",
       "      <td>reducing noise with indow #teamtrees &amp; lumber ...</td>\n",
       "      <td>[reduce, noise, indow, teamtrees, lumber, jack...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           videoId                                               text  \\\n",
       "1     :-5VBe9i4HDg  live tree planting counter - teamtrees [20 mil...   \n",
       "3     :-99jZt79aWk  why are trees so awesome #teamtrees nanodots m...   \n",
       "4     :-BqH2PKqFpM  the tree face #teamtrees robisplays teamtrees ...   \n",
       "5     :-BRWSaYQhM0                    donate to teamtrees #teamtrees    \n",
       "7     :-cPdImejxEQ  planting 20000000 trees will actually have thi...   \n",
       "...            ...                                                ...   \n",
       "1437  :_rQrNfRww60  hallowtree | donate to team trees | Animation ...   \n",
       "1439  :_vQ7OeVWBrU  let's stream for teamtrees gaming pixelfire ai...   \n",
       "1442  :_yKFJm75qO0  planting 20 million trees in prison architect ...   \n",
       "1443  :_zT8aOvNLaY  asmr | bob ross inspired painting asmr for #te...   \n",
       "1444  :__8RrX7Z8iY  reducing noise with indow #teamtrees & lumber ...   \n",
       "\n",
       "                                                 tokens  \n",
       "1     [live, tree, plant, counter, teamtrees, millio...  \n",
       "3     [tree, awesome, teamtrees, nanodots, magnets, ...  \n",
       "4     [tree, face, teamtrees, robisplays, teamtrees,...  \n",
       "5                        [donate, teamtrees, teamtrees]  \n",
       "7     [plant, 20000000, tree, actually, impact, mrbe...  \n",
       "...                                                 ...  \n",
       "1437  [hallowtree, donate, team, tree, animation, ha...  \n",
       "1439  [let, stream, teamtrees, game, pixelfire, code...  \n",
       "1442  [plant, million, tree, prison, architect, simu...  \n",
       "1443  [asmr, bob, ross, inspire, paint, asmr, teamtr...  \n",
       "1444  [reduce, noise, indow, teamtrees, lumber, jack...  \n",
       "\n",
       "[937 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gen tokens for each video. merge title and tags\n",
    "# Filter the data and generate token lists\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def isEnglish(s):\n",
    "    s=deEmojify(s)\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def process_text(txt):\n",
    "    result=str(txt)\n",
    "    soup = BeautifulSoup(result)\n",
    "    result=soup.get_text()\n",
    "    result=re.sub('[,\\.!?]', '', str(result))\n",
    "    result=result.lower()\n",
    "    return result\n",
    "\n",
    "def process_tag(tags):\n",
    "    result = tags.strip('][').split(',') \n",
    "    result=[r.strip('\\' ') for r in result]\n",
    "    result=\" \".join(result)\n",
    "    return result\n",
    "\n",
    "def lemmatize_stemming(tokens):\n",
    "    tks=[t.lower() for t in tokens if len(t)>=3]\n",
    "    result=filter(lambda t: t not in STOPWORDS, tks)\n",
    "    result=[lemmatizer.lemmatize(tk, pos='v') for tk in result]\n",
    "#     result=[stemmer.stem(tk) for tk in result]\n",
    "    return result\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"YouTube_CSV/video_FINAL_merged_teamtrees.csv\")\n",
    "print(len(df))    \n",
    "sdf=df.loc[df.country.isin([\"US\", \"CA\", \"GB\", \"AU\", \"NZ\", \"unknown\"])]# only in English speaking countries\n",
    "# sdf=df.loc[df.country.isin([\"US\", \"CA\", \"GB\", \"AU\", \"unknown\"])].iloc[:5000,:]# only in English speaking countries\n",
    "texts=sdf.title.apply(process_text)\n",
    "tags=sdf.tags.apply(process_tag)\n",
    "corpus=[(text+\" \"+tag) for text,tag in zip(texts, tags)]\n",
    "corpus_df=pd.DataFrame(data={\"videoId\":sdf[\"videoId\"],\"text\":corpus})\n",
    "corpus_df=corpus_df.loc[corpus_df.text.apply(isEnglish)] # must by English\n",
    "corpus_df=corpus_df.loc[corpus_df.text.apply(lambda x: \"teamtrees\" in x.lower())] # must have asmr\n",
    "\n",
    "corpus_df[\"tokens\"]=corpus_df.text.apply(word_tokenize)\n",
    "corpus_df[\"tokens\"]=corpus_df.tokens.apply(lemmatize_stemming) # lower the token and lemmatize\n",
    "bigram = Phrases(corpus_df.tokens, min_count=len(corpus_df)*0.05, delimiter=b'_')\n",
    "corpus_df[\"tokens\"]=corpus_df.tokens.apply(lambda tk: bigram[tk])\n",
    "\n",
    "# Save the dictionary\n",
    "dictionary = gensim.corpora.Dictionary(corpus_df[\"tokens\"])\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "dictionary.save(\"dictionary\")\n",
    "bow=list(corpus_df.tokens.apply(lambda tks:dictionary.doc2bow(tks)))\n",
    "\n",
    "corpus_df.to_csv(\"YouTube_CSV/corpus_data.csv\", index=False)\n",
    "\n",
    "print(len(corpus_df))\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "ROUND = 3\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "#read tagged urls\n",
    "df_tagged = pd.read_csv(\"Snow_Tagging/snowtagged.csv\")\n",
    "assert(df_tagged['videoId'].is_unique)\n",
    "print(len(df_tagged))\n",
    "tagged_id_list = [i for i in df_tagged[\"videoId\"]]\n",
    "\n",
    "videoid = list(corpus_df[\"videoId\"])\n",
    "videoid = [i for i in videoid if i is not \"#NAME?\" and i not in tagged_id_list]\n",
    "random.shuffle(videoid)\n",
    "\n",
    "\n",
    "#read from final data and shuffle video id list, removing invalid id and tagged id\n",
    "df = pd.read_csv(\"YouTube_CSV/video_FINAL_merged_teamtrees.csv\")\n",
    "\n",
    "cols = [\"embedUrl\",\n",
    "       \"videoId\",\n",
    "       \"title\",\n",
    "       \"isIncluded\",\n",
    "       \"diagnostic\",\n",
    "       \"prognostic\",\n",
    "       \"motivational\"]\n",
    "\n",
    "#convert to df and save to csv, add them to snowtagged.csv\n",
    "cat_ids = [i for i in videoid[:n]]\n",
    "# niu_ids = [i for i in videoid[n:n*2]]\n",
    "# kathy_ids = [i for i in videoid[n*2:n*3]]\n",
    "\n",
    "def generate_df(id_list):\n",
    "    data = []\n",
    "    embedUrl = [\"https://www.youtube.com/embed/\"+i[1:] for i in id_list]\n",
    "    title = [str(df.loc[df[\"videoId\"]==i][\"title\"].item()) for i in id_list]\n",
    "    videoId = [i for i in id_list]\n",
    "    data = [embedUrl, videoId, title, [], [], [], []]\n",
    "    return data\n",
    "\n",
    "df_cat = pd.DataFrame(generate_df(cat_ids)).transpose()\n",
    "# df_niu = pd.DataFrame([[\"https://www.youtube.com/watch?v=\"+i for i in niu_ids],[],[],[]]).transpose()\n",
    "# df_kathy = pd.DataFrame([[\"https://www.youtube.com/watch?v=\"+i for i in kathy_ids],[],[],[]]).transpose()\n",
    "df_cat.columns = cols\n",
    "# df_niu.columns = cols\n",
    "# df_kathy.columns = cols\n",
    "df_cat.to_csv(f\"Snow_Tagging/SnowTagging_Cat_ROUND{ROUND}.csv\", index=False)\n",
    "df_cat.to_csv(f\"Snow_Tagging/SnowTagging_Niu_ROUND{ROUND}.csv\", index=False)\n",
    "df_cat.to_csv(f\"Snow_Tagging/SnowTagging_Kathy_ROUND{ROUND}.csv\", index=False)\n",
    "# df_niu.to_csv(f\"SnowTagging_Niu_ROUND{ROUND}.csv\", index=False)\n",
    "# df_kathy.to_csv(f\"SnowTagging_Kathy_ROUND{ROUND}.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_urls = [\"https://www.youtube.com/embed/\"+i for i in cat_ids]\n",
    "new_ids = [i for i in cat_ids]\n",
    "df_new = pd.DataFrame([new_urls,new_ids]).transpose()\n",
    "df_new.columns = [\"video_url\",\"videoId\"]\n",
    "df_new.to_csv(\"Snow_Tagging/snowtagged.csv\", mode='a', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
